# ğŸ“Š PredicciÃ³n de CancelaciÃ³n de Clientes (Churn)

## ğŸ§‘â€ğŸ’» Autor 
**Nombre:** Alejandro Javier Contreras Olate

---

## ğŸ¯ DescripciÃ³n del Proyecto

Este proyecto aplica tÃ©cnicas de ciencia de datos y machine learning para **predecir quÃ© clientes estÃ¡n en riesgo de cancelar un servicio de telecomunicaciones**. Utiliza un enfoque progresivo en tres etapas: anÃ¡lisis exploratorio, modelado inicial y optimizaciÃ³n avanzada.

El objetivo es desarrollar un modelo de machine learning capaz de:
- Detectar con alta precisiÃ³n quÃ© clientes estÃ¡n en riesgo de cancelar
- Ayudar al negocio a tomar **decisiones proactivas** para retener clientes
- Implementar un pipeline de trabajo replicable para problemas de churn

---

## ğŸš€ Dataset

- [Telco Customer Churn - Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

---

## ğŸ§  TecnologÃ­as Utilizadas

- **Python 3.10**
- **Jupyter Notebooks**
- **pandas, numpy, matplotlib, seaborn**
- **scikit-learn** (modelos base y mÃ©tricas)
- **imbalanced-learn** (SMOTE)
- **XGBoost** (modelo optimizado)

---

## ğŸ—ï¸ Estructura del Proyecto

```bash
.
â”œâ”€â”€ 01_EDA.ipynb                            # ExploraciÃ³n de datos
â”œâ”€â”€ 02_Modeling.ipynb                       # Modelos base: Logistic Regression, Random Forest
â”œâ”€â”€ 03_OptimizacionAvanzada_SMOTE_XGBoost.ipynb  # SMOTE + XGBoost + mÃ©tricas avanzadas
â”œâ”€â”€ requirements.txt                        # LibrerÃ­as necesarias
â”œâ”€â”€ .gitignore                              # Ignora venv y archivos temporales
â””â”€â”€ README.md
```

---

## ğŸ“ DescripciÃ³n de los Notebooks

### `01_EDA.ipynb`
- Limpieza de datos y detecciÃ³n de valores faltantes
- AnÃ¡lisis de distribuciÃ³n de variables numÃ©ricas y categÃ³ricas
- VisualizaciÃ³n de correlaciones con la variable objetivo (Churn)
- IdentificaciÃ³n de patrones relevantes

### `02_Modeling.ipynb`
- CodificaciÃ³n de variables categÃ³ricas
- DivisiÃ³n de datos en entrenamiento y prueba
- Entrenamiento de modelos base: Logistic Regression y Random Forest
- EvaluaciÃ³n con mÃ©tricas: precisiÃ³n, recall, F1, matriz de confusiÃ³n

### `03_OptimizacionAvanzada_SMOTE_XGBoost.ipynb`
- AplicaciÃ³n de SMOTE para balancear clases desbalanceadas
- Entrenamiento de un modelo avanzado con XGBoost
- EvaluaciÃ³n con curva ROC, AUC, y anÃ¡lisis de importancia de variables
- ComparaciÃ³n entre modelos

---

## âš™ï¸ InstalaciÃ³n

### 1. Clona este repositorio:
```bash
git clone https://github.com/tu_usuario/proyecto-churn.git
cd proyecto-churn
```

### 2. Crea un entorno virtual (opcional pero recomendado):
```bash
python -m venv venv
venv\Scripts\activate     # Windows
source venv/bin/activate  # Mac/Linux
```

### 3. Instala las dependencias:
```bash
pip install -r requirements.txt
```

### 4. Ejecuta los notebooks con Jupyter o VS Code

---

## ğŸ“ˆ Resultados

- El modelo **XGBoost optimizado con SMOTE** logrÃ³ un **AUC superior a 0.85**, mostrando un excelente poder predictivo
- Se identificaron variables crÃ­ticas como el tipo de contrato, los cargos mensuales y la antigÃ¼edad del cliente
- Se generaron insights accionables para la retenciÃ³n de clientes

---

## ğŸ’¼ AplicaciÃ³n Empresarial

Este sistema de predicciÃ³n permite a una empresa:

- **Detectar clientes en riesgo** antes de que cancelen
- **Aplicar campaÃ±as de retenciÃ³n** personalizadas
- **Aumentar el ingreso promedio** por cliente (ARPU)
- **Reducir los costos de adquisiciÃ³n** de nuevos clientes

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - consulta el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor, abre un issue o envÃ­a un pull request para mejoras.

---

## ğŸ“§ Contacto

**Alejandro Javier Contreras Olate**
- GitHub: [@tu_usuario](https://github.com/tu_usuario)
- LinkedIn: [Tu perfil](https://linkedin.com/in/tu_perfil)


